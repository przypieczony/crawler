# This is a prototype of our first app
# Treat comments like pseduocode (https://en.wikipedia.org/wiki/Pseudocode)
# # Write tests for each function (http://www.onlamp.com/pub/a/python/2004/12/02/tdd_pyunit.html)

# We will be using 2 third-party libs: 
# - BeautifulSoup (bs4)
# - requests

# Yes, all code will be written in English ;)


# Regular expression (regexp in short) which finds web addresses 
# through out page. It may be usefull in future crawling.
# DONE

# Function which gets url link as a arg and reurns data page.
# Exclude .pdf/.jpg/.sth from fetch function
# This function must be well protected since it will be used many times.

# Improve validation of pages with too few links. For example use every link found
# on page even if required amount of links is not reached. Additionally if no links
# has found on page take another page to analyze from previous pool of pages

# Class which will find all links on page, returns only not yet visited
# and differentiate external links from internal ones. It should use regexp 
# from first paragrpah
# DONE

# Function which gets words as an input and returns dictionary of dictionaries:
# for example {page_addr: {word1: count1, word2: count2 ... }}
DONE

# Function which returns only words with most occurances and amount of 
# reurned words.
# Input: dict {page_addr: {word1: count1, word2: count2 ... }}
# 		 int 5
# Return: dict of tuple {page_addr: (word1, word2, workd3)}
# DONE

# Fix finding real words in standarizeWords and splitWord functions

# Add blacklist or whitelist of words to validate the word

# Function which saves dicts to file and format it as json (https://pl.wikipedia.org/wiki/JSON)

# Function which formats data for datamuse API

# Function which sends well formatted data to https://www.datamuse.com/api/

# Function which saves returned data from datamuse API  together with page addres to, again, json file

# Add logging

# Download up to external 5 pages deep. On each page another 5 pages should be analyzed.
# From each page gather 5 most frequent words with word occurences. Put this data
# to file together with page address. After that send this well formated data to
# https://www.datamuse.com/api/ API and save meaning to each page in a file.


